#!/usr/bin/env python3
"""
Performance Benchmarking for GRA Prototype
"""

import requests
import json
import time
import statistics
from concurrent.futures import ThreadPoolExecutor, as_completed

BASE_URL = "http://localhost:8000"

def benchmark_single_query():
    """Benchmark single query performance"""
    print("üöÄ Single Query Performance Benchmark")
    print("-" * 40)
    
    test_query = {
        "query": "What is RAG and how does it work in AI systems?",
        "context": "I'm learning about artificial intelligence"
    }
    
    response_times = []
    
    for i in range(10):
        start_time = time.time()
        
        try:
            response = requests.post(f"{BASE_URL}/query", json=test_query)
            end_time = time.time()
            
            if response.status_code == 200:
                response_time = end_time - start_time
                response_times.append(response_time)
                print(f"   Query {i+1}: {response_time:.3f}s")
            else:
                print(f"   Query {i+1}: Failed (HTTP {response.status_code})")
                
        except Exception as e:
            print(f"   Query {i+1}: Error - {e}")
    
    if response_times:
        avg_time = statistics.mean(response_times)
        min_time = min(response_times)
        max_time = max(response_times)
        std_dev = statistics.stdev(response_times) if len(response_times) > 1 else 0
        
        print(f"\nüìä Single Query Statistics:")
        print(f"   Average: {avg_time:.3f}s")
        print(f"   Minimum: {min_time:.3f}s")
        print(f"   Maximum: {max_time:.3f}s")
        print(f"   Std Dev: {std_dev:.3f}s")
        
        return avg_time
    
    return None

def benchmark_concurrent_queries(num_concurrent=5):
    """Benchmark concurrent query performance"""
    print(f"\n‚ö° Concurrent Queries Benchmark ({num_concurrent} concurrent)")
    print("-" * 40)
    
    test_query = {
        "query": "Explain simulation testing benefits",
        "context": "Performance testing context"
    }
    
    def send_query(query_id):
        start_time = time.time()
        try:
            response = requests.post(f"{BASE_URL}/query", json=test_query)
            end_time = time.time()
            
            return {
                "id": query_id,
                "success": response.status_code == 200,
                "response_time": end_time - start_time,
                "status_code": response.status_code
            }
        except Exception as e:
            return {
                "id": query_id,
                "success": False,
                "error": str(e),
                "response_time": None
            }
    
    start_time = time.time()
    
    with ThreadPoolExecutor(max_workers=num_concurrent) as executor:
        futures = [executor.submit(send_query, i) for i in range(num_concurrent)]
        results = [future.result() for future in as_completed(futures)]
    
    end_time = time.time()
    total_time = end_time - start_time
    
    successful_results = [r for r in results if r['success']]
    response_times = [r['response_time'] for r in successful_results]
    
    print(f"   Total Time: {total_time:.3f}s")
    print(f"   Successful Queries: {len(successful_results)}/{num_concurrent}")
    
    if response_times:
        avg_time = statistics.mean(response_times)
        min_time = min(response_times)
        max_time = max(response_times)
        
        print(f"   Average Response Time: {avg_time:.3f}s")
        print(f"   Min Response Time: {min_time:.3f}s")
        print(f"   Max Response Time: {max_time:.3f}s")
        print(f"   Throughput: {len(successful_results)/total_time:.2f} queries/second")
        
        return avg_time, len(successful_results)/total_time
    
    return None, 0

def benchmark_simulation_performance():
    """Benchmark simulation performance"""
    print(f"\nüéØ Simulation Performance Benchmark")
    print("-" * 40)
    
    scenarios = [
        {
            "name": "Basic Query",
            "scenario_type": "basic_query",
            "parameters": {"query_count": 5, "delay": 0.1}
        },
        {
            "name": "Stress Test",
            "scenario_type": "stress_test",
            "parameters": {"concurrent_users": 3, "queries_per_user": 5}
        },
        {
            "name": "Context Switching",
            "scenario_type": "context_switching",
            "parameters": {"context_switches": 5}
        }
    ]
    
    for scenario in scenarios:
        print(f"\n   üî¨ {scenario['name']} Simulation:")
        
        start_time = time.time()
        
        try:
            response = requests.post(f"{BASE_URL}/simulate", json={
                "scenario_type": scenario["scenario_type"],
                "parameters": scenario["parameters"]
            })
            
            end_time = time.time()
            simulation_time = end_time - start_time
            
            if response.status_code == 200:
                result = response.json()['result']
                print(f"      ‚úÖ Completed in {simulation_time:.3f}s")
                
                if 'success_rate' in result:
                    print(f"      üìä Success Rate: {result['success_rate']:.1%}")
                if 'average_response_time' in result:
                    print(f"      ‚è±Ô∏è  Avg Response Time: {result['average_response_time']:.3f}s")
            else:
                print(f"      ‚ùå Failed: HTTP {response.status_code}")
                
        except Exception as e:
            print(f"      ‚ùå Error: {e}")

def generate_performance_report():
    """Generate comprehensive performance report"""
    print("\n" + "=" * 60)
    print("üìã COMPREHENSIVE PERFORMANCE REPORT")
    print("=" * 60)
    
    # Single query benchmark
    single_query_avg = benchmark_single_query()
    
    # Concurrent query benchmarks
    concurrent_3_avg, throughput_3 = benchmark_concurrent_queries(3)
    concurrent_5_avg, throughput_5 = benchmark_concurrent_queries(5)
    concurrent_10_avg, throughput_10 = benchmark_concurrent_queries(10)
    
    # Simulation benchmarks
    benchmark_simulation_performance()
    
    # Summary
    print(f"\n" + "=" * 60)
    print("üéØ PERFORMANCE SUMMARY")
    print("=" * 60)
    
    if single_query_avg:
        print(f"üìä Single Query Performance: {single_query_avg:.3f}s average")
        
        if single_query_avg < 1.0:
            print("   üöÄ Rating: Excellent (< 1s)")
        elif single_query_avg < 2.0:
            print("   üëç Rating: Good (< 2s)")
        elif single_query_avg < 3.0:
            print("   ‚ö†Ô∏è  Rating: Acceptable (< 3s)")
        else:
            print("   üêå Rating: Needs Improvement (> 3s)")
    
    print(f"\n‚ö° Concurrent Performance:")
    if throughput_3 > 0:
        print(f"   3 concurrent: {throughput_3:.2f} queries/second")
    if throughput_5 > 0:
        print(f"   5 concurrent: {throughput_5:.2f} queries/second")
    if throughput_10 > 0:
        print(f"   10 concurrent: {throughput_10:.2f} queries/second")
    
    print(f"\nüéØ System Capabilities:")
    print(f"   ‚úÖ RAG Query Processing: Functional")
    print(f"   ‚úÖ Multiple Simulation Types: 4 scenarios available")
    print(f"   ‚úÖ Concurrent Request Handling: Supported")
    print(f"   ‚úÖ Performance Monitoring: Built-in metrics")
    
    print(f"\nüí° Recommendations:")
    if single_query_avg and single_query_avg > 2.0:
        print(f"   - Consider optimizing query processing pipeline")
        print(f"   - Review vector database performance")
    
    print(f"   - System is ready for production testing")
    print(f"   - Consider load balancing for high traffic")
    print(f"   - Monitor memory usage under sustained load")

if __name__ == "__main__":
    generate_performance_report()